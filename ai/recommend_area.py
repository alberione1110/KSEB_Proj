import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from sklearn.preprocessing import MinMaxScaler
import google.generativeai as genai
import pymysql
import json

def get_recommendation(gu_name, category_small) :
    # RDS ì •ë³´
    host = 'daktor-commercial-prod.czig88k8s0e8.ap-northeast-2.rds.amazonaws.com'
    port = 3306
    user = 'oesnue'
    password = 'gPwls0105!' #ì•ˆë˜ë©´ gPwls0105
    database = 'daktor_db'

    # ì—°ê²° ì‹œë„
    try:
        conn = pymysql.connect(
            host=host,
            user=user,
            password=password,
            database=database,
            port=port,
            connect_timeout=5
        )
        print("âœ… RDS ì—°ê²° ì„±ê³µ")

        # ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        with conn.cursor() as cursor:
            cursor.execute("SHOW TABLES;")
            for row in cursor.fetchall():
                print(row)

        conn.close()

    except Exception as e:
        print("âŒ ì—°ê²° ì‹¤íŒ¨:", e)

    # SQLAlchemy ì—”ì§„ ìƒì„±
    engine = create_engine(
        f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}',
        connect_args={'charset':'utf8mb4'}
    )

    # ê³µí†µ í•¨ìˆ˜: ìµœê·¼ Nê°œ ë¶„ê¸° ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (region_code, year, quarter í•„ë“œ ê¸°ì¤€)
    def get_recent_rows(df, group_col='region_code', num_quarters=4):
        df_sorted = df.sort_values(by=[group_col, 'year', 'quarter'])
        return df_sorted.groupby(group_col, group_keys=False).tail(num_quarters)

    # get_same_quarter_rows í•¨ìˆ˜ëŠ” í•„ìš”í•˜ë©´ ì¶”ê°€ ê°€ëŠ¥

    # 4. êµ¬ ì½”ë“œ(ì§€ì—­ ì½”ë“œ) ì¡°íšŒ (floating_population_stats í…Œì´ë¸” ì‚¬ìš© ì˜ˆì‹œ)
    sql_gu_code = f"""
        SELECT DISTINCT region_code
        FROM floating_population_stats
        WHERE region_name = '{gu_name}'
        LIMIT 1
    """

    gu_code_df = pd.read_sql(sql_gu_code, engine)
    gu_code = str(gu_code_df.iloc[0]['region_code'])
    print(f"ì„ íƒí•œ êµ¬ '{gu_name}'ì˜ ì§€ì—­ ì½”ë“œ: {gu_code}")
    print(f"ì„ íƒí•œ ì—…ì¢…: {category_small}")

    # 5. ê° í…Œì´ë¸”ë³„ ë°ì´í„° ì¡°íšŒ ì¿¼ë¦¬ í•¨ìˆ˜
    def query_table(table_name, extra_condition="", category_col='category_small', indicator_col='indicator', indicator_val=None):
        # ê¸°ë³¸ ì¡°ê±´: region_codeê°€ êµ¬ ì½”ë“œë¡œ ì‹œì‘
        condition = f"region_code LIKE '{gu_code}%%'"

        # ì—…ì¢… í•„í„°ë§ ì¡°ê±´ ì¶”ê°€
        if category_small and category_col:
            condition += f" AND {category_col} = '{category_small}'"
        
        # indicator í•„í„°ë§ ì¡°ê±´ ì¶”ê°€
        if indicator_val and indicator_col:
            condition += f" AND {indicator_col} = '{indicator_val}'"
        
        # ê¸°íƒ€ ì¡°ê±´ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if extra_condition:
            condition += f" AND {extra_condition}"
        
        sql = f"""
            SELECT *
            FROM {table_name}
            WHERE {condition}
            ORDER BY region_code, year, quarter
        """
        df = pd.read_sql(sql, engine)
        return df

    def load_table(table_name):
        try:
            return pd.read_sql(f"SELECT * FROM {table_name}", engine)
        except Exception as e:
            print(f"âŒ [ERROR] í…Œì´ë¸” {table_name} ë¡œë”© ì‹¤íŒ¨:", e)
            # ë¡¤ë°± ê°•ì œ ì‹¤í–‰
            engine.dispose()  # í˜„ì¬ ì—°ê²° ì™„ì „íˆ ì´ˆê¸°í™”
            raise e

    def query_sales(table):
        df = pd.read_sql(f"SELECT * FROM {table}", engine)
        df = df[['year','quarter','zone_id','zone_name','region_name','service_code','service_name','monthly_sales']]
        df['zone_id'] = df['zone_id'].astype(str)

        rent_test_df = rent_df[['region_name','region_code']].drop_duplicates(subset=['region_name', 'region_code'])
        df = df.merge(rent_test_df[['region_name', 'region_code']], on='region_name', how='left')
        df = df[df['region_code'].notna()]
        df = df[df['service_name'] == category_small]
        return df
        

    def add_region_service_names(df, zone_df, service_df, rent_df):

        # zone_df ê¸°ì¤€ í–‰ì •ë™ ì´ë¦„ ë¶™ì´ê¸°
        df['zone_id'] = df['zone_id'].astype(str)
        zone_df['zone_id'] = zone_df['zone_id'].astype(str)
        print("íƒ€ì…í™•ì¸",zone_df['zone_id'].dtype)
        df = df.merge(zone_df[['zone_id', 'region_name']], on='zone_id', how='left')
        rent_test_df = rent_df[['region_name','region_code']].drop_duplicates(subset=['region_name', 'region_code'])
        df = df.merge(rent_test_df[['region_name', 'region_code']], on='region_name', how='left')
        df = df[df['region_code'].notna()]
        df = df.merge(service_df[['service_code', 'service_name']], on='service_code', how='left')
        df = df[df['service_name'] == category_small+'\r']
        return df



    # 6. ê° ë°ì´í„° ì¡°íšŒ
    pop_df = query_table('floating_population_stats', category_col=None)  # ìœ ë™ì¸êµ¬ í…Œì´ë¸”ì—ëŠ” ì—…ì¢… ì—†ìŒ ê°€ì •
    rent_df = query_table('rental_price_stats', category_col=None)
    age_df = query_table('subcategory_avg_operating_period_stats', indicator_val='avg_operating_years_30')
    store_df = query_table('subcategory_store_count_stats', indicator_val='store_total')
    survive_df = query_table('subcategory_startup_survival')
    openclose_df = query_table('subcategory_openclose_stats')

    # ê¸°ë³¸ í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°
    zone_df = load_table('zone_table')
    zone_df['zone_id']= zone_df['zone_id'].astype(str)
    service_df = load_table('service_type')


    # 7. ìµœê·¼ 4ê°œ ë¶„ê¸° í•„í„°ë§
    pop_filtered = get_recent_rows(pop_df)[["region_name", "region_code", "year", "quarter", "floating_population"]]
    rent_filtered = get_recent_rows(rent_df)[["region_name", "region_code", "year", "quarter", "rent_total"]]

    age_filtered = get_recent_rows(age_df)[["category_small", "region_name", "region_code", "year", "quarter", "indicator", "value"]]
    store_filtered = get_recent_rows(store_df)[["category_small", "region_name", "region_code", "year", "quarter", "indicator", "value"]]

    survive_filtered = get_recent_rows(survive_df)[["category_small", "region_name", "region_code", "year", "quarter",
                                   "survival_1yr", "survival_3yr", "survival_5yr"]]
    openclose_filtered = openclose_df[["category_small", "region_name", "region_code", "year", "quarter",
                                       "num_open", "num_close"]]

    years = [2022, 2023, 2024]
    gender_sales = {}
    gender_sales_test ={}
    age_sales = {}
    summary_sales = {}
    # ë§¤ì¶œ ê´€ë ¨ ë°ì´í„° ì²˜ë¦¬
    for year in years:
        st_ct_df = load_table(f"zone_store_count_{year}") #zone ì—…ì¢…ë‹¹ ì í¬ìˆ˜
        gender_df = load_table(f"sales_by_gender_age_{year}")
        gender_known = gender_df[gender_df['gender'].isin(['ì—¬ì„±', 'ë‚¨ì„±'])]
        gender_unknown = gender_df[~gender_df['gender'].isin(['ì—¬ì„±', 'ë‚¨ì„±'])]

        gender_known = add_region_service_names(gender_known, zone_df, service_df, rent_df)
        gender_unknown = add_region_service_names(gender_unknown, zone_df, service_df, rent_df)
        
        #--gender
        gender_sales[year] = gender_known[["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "gender", "sales_amount"]] \
            .sort_values(by=["service_name", "gender", "year", "quarter"]).reset_index(drop=True)

        gender_sales[year] = gender_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        gender_sales[year].loc[:, 'avg_sales_per_store'] = gender_sales[year]['sales_amount'] / gender_sales[year]['count']

        gender_sales[year] = gender_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "gender", "avg_sales_per_store","count"]] \
            .sort_values(by=['region_name', 'year', 'quarter','gender']).reset_index(drop=True)
        # ì •ë ¬ ê¸°ì¤€: region_name > year > quarter
        
        #--age
        age_sales[year] = gender_unknown[["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "age_group", "sales_amount"]] \
            .sort_values(by=["service_name", "age_group", "year", "quarter"]).reset_index(drop=True)
        
        age_sales[year] = age_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        age_sales[year].loc[:, 'avg_sales_per_store'] = age_sales[year]['sales_amount'] / age_sales[year]['count']

        age_sales[year] = age_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "age_group", "avg_sales_per_store","count"]] \
            .sort_values(by=['region_name', 'year', 'quarter','age_group']).reset_index(drop=True)
        
        #--summary
        sales_df = query_sales(f"sales_summary_{year}")
        summary_sales[year] = sales_df[["region_name", "service_name", "zone_id", "service_code", "year", "quarter", "monthly_sales"]]
        summary_sales[year].loc[:, 'zone_id'] = summary_sales[year]['zone_id'].astype(str)

        summary_sales[year] = summary_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        summary_sales[year].loc[:, 'avg_sales_per_store'] = summary_sales[year]['monthly_sales'] / summary_sales[year]['count']

        summary_sales[year] = summary_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "avg_sales_per_store","count"]] \
            .sort_values(by=['region_name', 'year', 'quarter']).reset_index(drop=True)
        
    for year in years:
        print(f"{year}",gender_sales[year])
        print(f"{year}",age_sales[year])
        print(f"{year}",summary_sales[year])

    # í‰ê· ê°’ ê³„ì‚° í•¨ìˆ˜
    def get_avg(df, group_col='region_code', val_col=None, rename_col=None):
        df[val_col] = pd.to_numeric(df[val_col], errors='coerce')
        df = df.dropna(subset=[val_col])
        avg = df.groupby(group_col).agg({
            val_col: 'mean',
            'region_name': 'first'
        }).round(2).reset_index()
        if rename_col:
            avg = avg.rename(columns={val_col: rename_col})
        return avg

    # í‰ê·  ê³„ì‚°
    pop_avg = get_avg(pop_filtered, val_col='floating_population', rename_col='ìœ ë™ì¸êµ¬')
    rent_avg = get_avg(rent_filtered, val_col='rent_total', rename_col='ì„ëŒ€ì‹œì„¸')
    age_avg = get_avg(age_filtered, val_col='value', rename_col='í‰ê· ì˜ì—…ê¸°ê°„(ë…„)')
    store_avg = get_avg(store_filtered, val_col='value', rename_col='ì í¬ìˆ˜')

    survive_avg = survive_filtered.groupby('region_code').agg({
        'survival_1yr': 'mean',
        'survival_3yr': 'mean',
        'survival_5yr': 'mean',
        'region_name': 'first'
    }).round(2).reset_index().rename(columns={
        'survival_1yr': '1ë…„ ìƒì¡´ìœ¨(%)',
        'survival_3yr': '3ë…„ ìƒì¡´ìœ¨(%)',
        'survival_5yr': '5ë…„ ìƒì¡´ìœ¨(%)'
    })

    openclose_avg = openclose_filtered.groupby('region_code').agg({
        'num_open': 'mean',
        'num_close': 'mean',
        'region_name': 'first'
    }).round(2).reset_index().rename(columns={
        'num_open': 'í‰ê·  ê°œì—…ìˆ˜',
        'num_close': 'í‰ê·  íì—…ìˆ˜'
    })

    # ë³‘í•©
    dfs = [pop_avg, rent_avg, age_avg, store_avg, survive_avg, openclose_avg]
    from functools import reduce
    merged_df = reduce(lambda left, right: pd.merge(left, right, on=['region_code', 'region_name']), dfs)

    # ì»¬ëŸ¼ ì •ë¦¬
    merged_df = merged_df.rename(columns={
        'region_code': 'í–‰ì •ë™ì½”ë“œ',
        'region_name': 'í–‰ì •ë™ëª…'
    }).sort_values(by='ìœ ë™ì¸êµ¬', ascending=False).reset_index(drop=True)

    # ì¶œë ¥
    pd.set_option('display.float_format', '{:,.2f}'.format)
    print("ğŸ“Š ì¢…í•© ì§€ì—­ ìƒê¶Œ ìš”ì•½ ë¦¬í¬íŠ¸")
    print(merged_df)


    def get_avg_sales(sales_df, group_col):
        sales_df['avg_sales_per_store'] = pd.to_numeric(sales_df['avg_sales_per_store'], errors='coerce')
        sales_df = sales_df.dropna(subset=['avg_sales_per_store'])
        
        avg_df = sales_df.groupby(['region_name', group_col]).agg({
            'avg_sales_per_store': 'mean',
            #'region_name': 'first',
            'service_name': 'first'
        }).round(2).reset_index()

        avg_df['avg_sales_per_store'] = (avg_df['avg_sales_per_store'] / 3).round(2)

        return avg_df

    def get_avg_sales_sum(sales_df, group_col=None):
        # ë§¤ì¶œ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜, ê²°ì¸¡ì¹˜ ì œê±°
        sales_df['avg_sales_per_store'] = pd.to_numeric(sales_df['avg_sales_per_store'], errors='coerce')
        sales_df = sales_df.dropna(subset=['avg_sales_per_store'])

        # ê¸°ë³¸ ê·¸ë£¹í•‘ ì»¬ëŸ¼: ì—…ì¢… ì½”ë“œ
        group_cols = ['region_name']
        #if group_col:
        #    group_cols.append(group_col)  # ì¶”ê°€ ê·¸ë£¹í•‘ ì»¬ëŸ¼ (ì˜ˆ: ì§€ì—­ì½”ë“œ ë“±)

        # ê·¸ë£¹ë³„ í‰ê·  ë§¤ì¶œ ê³„ì‚°
        avg_df = sales_df.groupby(group_cols).agg({
            'avg_sales_per_store': 'mean',     # ë¶„ê¸°ë³„ í‰ê·  ë§¤ì¶œ
            'region_name': 'first',            # ëŒ€í‘œ ì§€ì—­ëª… (group_colì´ ì§€ì—­ì¼ ê²½ìš°)
            'service_name': 'first'            # ì—…ì¢…ëª…
        })

        # ë¶„ê¸° ë§¤ì¶œ â†’ ì›”í‰ê·  ë§¤ì¶œë¡œ í™˜ì‚°
        avg_df['avg_sales_per_store'] = (avg_df['avg_sales_per_store'] / 3).round(2)

        return avg_df


    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    gender_list = []
    age_list = []
    summary_list = []

    for year in years:
        # ì„±ë³„ë³„ í‰ê· 
        gender_avg_df = get_avg_sales(gender_sales[year], group_col='gender')
        gender_avg_df['year'] = year
        gender_list.append(gender_avg_df)

        # ì—°ë ¹ëŒ€ë³„ í‰ê· 
        age_avg_df = get_avg_sales(age_sales[year], group_col='age_group')
        age_avg_df['year'] = year
        age_list.append(age_avg_df)

        # ì „ì²´ ì—…ì¢… í‰ê· 
        summary_avg_df = get_avg_sales_sum(summary_sales[year], group_col='service_code')
        summary_avg_df['year'] = year
        summary_list.append(summary_avg_df)

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹˜ê¸°
    gender_df = pd.concat(gender_list).reset_index(drop=True)
    age_df = pd.concat(age_list).reset_index(drop=True)
    summary_df = pd.concat(summary_list).reset_index(drop=True)

    # ì»¬ëŸ¼ëª… ì •ë¦¬
    gender_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'gender': 'ì„±ë³„',
        'avg_sales_per_store': 'ì„±ë³„ë³„ í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    age_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'age_group': 'ì—°ë ¹ëŒ€',
        'avg_sales_per_store': 'ì—°ë ¹ëŒ€ë³„ í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    summary_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'avg_sales_per_store': 'í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    gender_df.to_json('gender_avg_sales_dong.json', orient='records', force_ascii=False, indent=4)
    age_df.to_json('age_avg_sales_dong.json', orient='records', force_ascii=False, indent=4)
    summary_df.to_json('summary_avg_sales_dong.json', orient='records', force_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ ë³€ìˆ˜ì— ì €ì¥
    gender_dong_json = gender_df.to_json(orient='records', force_ascii=False, indent=4)
    age_dong_json = age_df.to_json(orient='records', force_ascii=False, indent=4)
    summary_dong_json = summary_df.to_json(orient='records', force_ascii=False, indent=4)


    # ------------------------
    # ì •ê·œí™” ëŒ€ìƒ ì»¬ëŸ¼ ì •ì˜
    # ------------------------
    score_columns = [
        'ìœ ë™ì¸êµ¬', 'ì„ëŒ€ì‹œì„¸', 'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)', 'ì í¬ìˆ˜',
        '1ë…„ ìƒì¡´ìœ¨(%)', '3ë…„ ìƒì¡´ìœ¨(%)', '5ë…„ ìƒì¡´ìœ¨(%)',
        'í‰ê·  ê°œì—…ìˆ˜', 'í‰ê·  íì—…ìˆ˜'
    ]
    score_columns_2 = ['2022_í‰ê· ë§¤ì¶œ', '2023_í‰ê· ë§¤ì¶œ', '2024_í‰ê· ë§¤ì¶œ']

    # ------------------------
    # ê°€ì¤‘ì¹˜ ì •ì˜ (ìˆ˜ì • ê°€ëŠ¥)
    # ------------------------
    weights = {
        'ìœ ë™ì¸êµ¬': 0.1483,
        'ì„ëŒ€ì‹œì„¸': -0.0909,
        'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)': 0.0574,
        'ì í¬ìˆ˜': 0.2871,
        '1ë…„ ìƒì¡´ìœ¨(%)': 0.0574,
        '3ë…„ ìƒì¡´ìœ¨(%)': 0.0574,
        '5ë…„ ìƒì¡´ìœ¨(%)': 0.0574,
        'í‰ê·  ê°œì—…ìˆ˜': 0.0287,
        'í‰ê·  íì—…ìˆ˜': -0.0287,
        '2022_í‰ê· ë§¤ì¶œ': 0.0718,
        '2023_í‰ê· ë§¤ì¶œ': 0.1053,
        '2024_í‰ê· ë§¤ì¶œ': 0.1053
    }

    ## ------------------------
    # ì •ê·œí™” ìˆ˜í–‰
    # ------------------------
    # summary_dfì—ì„œ ì—°ë„ë³„ í‰ê·  ë§¤ì¶œ í”¼ë²— (ì—…ì¢…/í–‰ì •ë™ ê¸°ì¤€ìœ¼ë¡œ wide format ë§Œë“¤ê¸°)
    pivot_summary = summary_df.pivot_table(
        index=['í–‰ì •ë™ëª…', 'ì—…ì¢…ëª…'],
        columns='year',
        values='í‰ê·  ì›” ë§¤ì¶œ',
        aggfunc='mean'
    ).reset_index()

    # ì»¬ëŸ¼ëª… ì •ë¦¬ (2022, 2023, 2024 â†’ '2022_í‰ê· ë§¤ì¶œ' í˜•íƒœë¡œ)
    pivot_summary.columns.name = None
    pivot_summary.rename(columns={
        2022: '2022_í‰ê· ë§¤ì¶œ',
        2023: '2023_í‰ê· ë§¤ì¶œ',
        2024: '2024_í‰ê· ë§¤ì¶œ'
    }, inplace=True)

    # merged_dfì— í‰ê·  ë§¤ì¶œ ë³‘í•©
    # ë³‘í•© ì „ì— ì»¬ëŸ¼ëª… ë§ì¶”ê¸° (í•´ë‹¹ ì—†ìœ¼ë©´ ìƒëµ ê°€ëŠ¥)
    merged_df = merged_df.merge(pivot_summary, on=['í–‰ì •ë™ëª…'], how='left')
    merged_df.to_json('merged_dong.json', orient='records', force_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ ë³€ìˆ˜ì— ì €ì¥
    merged_dong_json = merged_df.to_json(orient='records', force_ascii=False, indent=4)

    # ì •ê·œí™” ëŒ€ìƒ ì»¬ëŸ¼
    all_score_columns = score_columns + score_columns_2

    # ì •ê·œí™” ìˆ˜í–‰
    clean_df = merged_df[all_score_columns].replace([np.inf, -np.inf], np.nan).fillna(0)

    scaler = MinMaxScaler()
    normalized = scaler.fit_transform(clean_df)
    normalized_df = pd.DataFrame(normalized, columns=[f'norm_{col}' for col in all_score_columns])
    # ë³‘í•©
    merged_with_norm = pd.concat([merged_df, normalized_df], axis=1)

    # ì ìˆ˜ ê³„ì‚°
    merged_with_norm['í–‰ì •ë™_ì¶”ì²œì ìˆ˜'] = sum(
        merged_with_norm[f'norm_{col}'] * weight
        for col, weight in weights.items()
    )

    # ------------------------
    # ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥
    # ------------------------

    # ì •ê·œí™”ëœ ì»¬ëŸ¼ ì´ë¦„ ì •ì˜
    norm_cols = [f'norm_{col}' for col in all_score_columns]

    # ì§€ì—­ ì ìˆ˜ì™€ ì •ê·œí™” ì»¬ëŸ¼ë“¤ì„ í•¨ê»˜ ê²°í•©
    final_result = merged_with_norm.drop(columns=norm_cols).sort_values(by='í–‰ì •ë™_ì¶”ì²œì ìˆ˜', ascending=False).reset_index(drop=True)
    final_result = final_result[final_result['í–‰ì •ë™ì½”ë“œ'].astype(str).str.len() != 5]
    final_result = final_result.drop(columns='ì—…ì¢…ëª…')
    # ê²°ì¸¡ ë˜ëŠ” ë¬´í•œëŒ€ í¬í•¨ëœ í–‰ì •ë™ ì œì™¸
    final_result = final_result.replace([np.inf, -np.inf], np.nan)
    final_result = final_result.dropna(subset=['2022_í‰ê· ë§¤ì¶œ', '2023_í‰ê· ë§¤ì¶œ', '2024_í‰ê· ë§¤ì¶œ'])

    # ì €ì¥
    final_result.to_csv('filtered_result_dong.csv', index=False, encoding='utf-8-sig')
    final_result.to_json('filtered_result_dong.json', orient='records', force_ascii=False, indent=4)
    filtered_result_dong_json = final_result.to_json(orient='records', force_ascii=False, indent=4)


    # ê²°ê³¼ í™•ì¸
    print("ğŸ† ìµœì¢… ì¶”ì²œ í–‰ì •ë™ (ì›ë³¸ ì§€í‘œ + ì§€ì—­ ì ìˆ˜ í¬í•¨)")
    print("ì¶”ì²œ í–‰ì •ë™\n",final_result)
    print("í–‰ì •ë™ëª… ë° ì ìˆ˜\n",final_result[['í–‰ì •ë™ëª…','í–‰ì •ë™_ì¶”ì²œì ìˆ˜']].head(10))

    #----LLM----
    # API í‚¤ ì„¤ì •
    genai.configure(api_key="AIzaSyCiEbjep2f6PRLqTr1JKYE2vMlbrAHvr-E")

    # ëª¨ë¸ ì„ íƒ
    model = genai.GenerativeModel(model_name="models/gemini-2.5-flash")
    def generate_region_summary(row, gu_name, category_small):
        few_shot_examples = """
    [ì˜ˆì‹œ 1]
    - í–‰ì •ë™ëª…: ì„œêµë™
    - ìœ ë™ì¸êµ¬: 25,000ëª…
    - ì„ëŒ€ì‹œì„¸: 3,200,000ì›/í‰
    - í‰ê· ì˜ì—…ê¸°ê°„: 4.2ë…„
    - ì í¬ìˆ˜: 180ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: 72%
    - 5ë…„ ìƒì¡´ìœ¨: 55%
    - í‰ê·  ê°œì—…ìˆ˜: 14ê°œ
    - í‰ê·  íì—…ìˆ˜: 8ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: 8,400,000ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: 8,800,000ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: 9,200,000ì›

    ì„œêµë™ì€ ë†’ì€ ìœ ë™ì¸êµ¬ì™€ ì§€ì†ì ìœ¼ë¡œ ìƒìŠ¹í•˜ëŠ” í‰ê·  ë§¤ì¶œ íë¦„ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. ë˜í•œ ì í¬ ìˆ˜ê°€ ë§ê³  ìƒì¡´ìœ¨ë„ ìš°ìˆ˜í•´ ì»¤í”¼ ì „ë¬¸ì  ì°½ì—…ì— ì•ˆì •ì ì¸ ì¡°ê±´ì„ ê°–ì¶˜ ìƒê¶Œì…ë‹ˆë‹¤.

    [ì˜ˆì‹œ 2]
    - í–‰ì •ë™ëª…: í–‰ë‹¹ë™
    - ìœ ë™ì¸êµ¬: 15,000ëª…
    - ì„ëŒ€ì‹œì„¸: 2,100,000ì›/í‰
    - í‰ê· ì˜ì—…ê¸°ê°„: 3.6ë…„
    - ì í¬ìˆ˜: 110ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: 69%
    - 5ë…„ ìƒì¡´ìœ¨: 50%
    - í‰ê·  ê°œì—…ìˆ˜: 10ê°œ
    - í‰ê·  íì—…ìˆ˜: 6ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: 6,400,000ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: 2,800,000ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: 7,100,000ì›

    í–‰ë‹¹ë™ì€ ì„ëŒ€ì‹œì„¸ê°€ ë‚®ì€ í¸ì´ë©´ì„œë„ ë§¤ì¶œ íë¦„ì´ ì•ˆì •ì ì´ê³ , ìƒì¡´ìœ¨ë„ í‰ê·  ì´ìƒì…ë‹ˆë‹¤. ì˜ˆì‚°ì´ ì œí•œì ì¸ ì´ˆê¸° ì°½ì—…ìì—ê²Œ íŠ¹íˆ ìœ ë¦¬í•œ ì…ì§€ì…ë‹ˆë‹¤.
    """

        # í”„ë¡¬í”„íŠ¸ ì™„ì„±
        prompt = f"""
    ë‹¹ì‹ ì€ ìƒê¶Œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” '{gu_name}' ì§€ì—­ '{category_small}' ì—…ì¢…ì˜ íŠ¹ì • í–‰ì •ë™ í•µì‹¬ ìƒê¶Œ ì§€í‘œì…ë‹ˆë‹¤:

    ë¨¼ì € ì°¸ê³ ìš© ì˜ˆì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”:
    {few_shot_examples}

    ---

    ì´ì œ ì•„ë˜ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§€ì—­ì˜ ì¶”ì²œ ì‚¬ìœ ë¥¼ ê¸ì •ì ìœ¼ë¡œ **3ì¤„ ì´ë‚´ë¡œ ì‘ì„±**í•´ ì£¼ì„¸ìš”:

    - í–‰ì •ë™ëª…: {row['í–‰ì •ë™ëª…']}
    - ìœ ë™ì¸êµ¬: {int(row['ìœ ë™ì¸êµ¬']):,}ëª…
    - ì„ëŒ€ì‹œì„¸: {int(row['ì„ëŒ€ì‹œì„¸']):,}ì›/í‰
    - í‰ê· ì˜ì—…ê¸°ê°„: {row['í‰ê· ì˜ì—…ê¸°ê°„(ë…„)']}ë…„
    - ì í¬ìˆ˜: {row['ì í¬ìˆ˜']}ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: {row['3ë…„ ìƒì¡´ìœ¨(%)']}%
    - 5ë…„ ìƒì¡´ìœ¨: {row['5ë…„ ìƒì¡´ìœ¨(%)']}%
    - í‰ê·  ê°œì—…ìˆ˜: {row['í‰ê·  ê°œì—…ìˆ˜']}ê°œ
    - í‰ê·  íì—…ìˆ˜: {row['í‰ê·  íì—…ìˆ˜']}ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2022_í‰ê· ë§¤ì¶œ']):,}ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2023_í‰ê· ë§¤ì¶œ']):,}ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2024_í‰ê· ë§¤ì¶œ']):,}ì›


    """
        response = model.generate_content(prompt)
        return response.text.strip()

    recommendation_list = []

    # ìƒìœ„ 5ê°œ í–‰ì •ë™
    top_n = 5

    for idx, row in final_result.head(top_n).iterrows():
        full_district_name = f"{gu_name} {row['í–‰ì •ë™ëª…']}"
        reason = generate_region_summary(row, gu_name, category_small)

        recommendation_list.append({
            'district': full_district_name,
            'reason': reason
        })

    recommendation_dict = {
        category_small: recommendation_list
    }

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open('recommendation_dong.json', 'w', encoding='utf-8') as f:
        json.dump(recommendation_dict, f, ensure_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ë¡œ ë³€ìˆ˜ì— ì €ì¥
    recommendation_dong_json = json.dumps(recommendation_dict, ensure_ascii=False, indent=4)

    # ì¶œë ¥
    print(recommendation_dong_json)

