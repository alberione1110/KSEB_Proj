import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from functools import reduce
from sklearn.preprocessing import MinMaxScaler
import google.generativeai as genai
import pymysql
import json

def run_industry_recommendation(region, gu_name) :
    # RDS ì •ë³´
    host = 'daktor-commercial-prod.czig88k8s0e8.ap-northeast-2.rds.amazonaws.com'
    port = 3306
    user = 'oesnue'
    password = 'gPwls0105!' #ì•ˆë˜ë©´ gPwls0105
    database = 'daktor_db'

    # ì—°ê²° ì‹œë„
    try:
        conn = pymysql.connect(
            host=host,
            user=user,
            password=password,
            database=database,
            port=port,
            connect_timeout=5
        )
        print("âœ… RDS ì—°ê²° ì„±ê³µ")

        # ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        with conn.cursor() as cursor:
            cursor.execute("SHOW TABLES;")
            for row in cursor.fetchall():
                print(row)

        conn.close()

    except Exception as e:
        print("âŒ ì—°ê²° ì‹¤íŒ¨:", e)


    # SQLAlchemy ì—”ì§„ ìƒì„±
    engine = create_engine(
        f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}',
        connect_args={'charset':'utf8mb4'}
    )

    # ê³µí†µ ìœ í‹¸ í•¨ìˆ˜
    def get_recent_quarters_by_category(df, group_cols=['category_small'], num_quarters=4):
        df_sorted = df.sort_values(by=group_cols + ['year', 'quarter'])
        return df_sorted.groupby(group_cols, group_keys=False).tail(num_quarters)

    def load_table(table_name):
        try:
            return pd.read_sql(f"SELECT * FROM {table_name}", engine)
        except Exception as e:
            print(f"âŒ [ERROR] í…Œì´ë¸” {table_name} ë¡œë”© ì‹¤íŒ¨:", e)
            # ë¡¤ë°± ê°•ì œ ì‹¤í–‰
            engine.dispose()  # í˜„ì¬ ì—°ê²° ì™„ì „íˆ ì´ˆê¸°í™”
            raise e
        
    def query_table(table_name, extra_condition="", category_col=None, indicator_col='indicator', indicator_val=None):
        use_region_code = 'region_code' in pd.read_sql(f"SHOW COLUMNS FROM {table_name}", engine)['Field'].values
        conditions = []

        if use_region_code:
            conditions.append(f"region_code LIKE '{dong_code}%%'")
        elif target_dong and category_col:
            conditions.append(f"{category_col} = '{target_dong}'")
        if indicator_val and indicator_col:
            conditions.append(f"{indicator_col} = '{indicator_val}'")
        if extra_condition:
            conditions.append(extra_condition)

        where_clause = " AND ".join(conditions) if conditions else "1=1"
        sql = f"SELECT * FROM {table_name} WHERE {where_clause} ORDER BY year, quarter"
        return pd.read_sql(sql, engine)

    def query_sales(table, service_code=None):
        cond = f"region_name = '{target_dong}'"
        if service_code:
            cond += f" AND service_code = '{service_code}'"
        sql = f"SELECT * FROM {table} WHERE {cond} ORDER BY service_code, zone_id, year, quarter"
        df = pd.read_sql(sql, engine)
        return df.drop_duplicates(subset=["region_name", "year", "quarter", "service_code", "monthly_sales"])

    def add_region_service_names(df, zone_df, service_df, target_dong=None):
        df['zone_id'] = df['zone_id'].astype(str)
        zone_df['zone_id'] = zone_df['zone_id'].astype(str)

        df = df.merge(zone_df, on='zone_id', how='left')
        df = df.merge(service_df, on='service_code', how='left')
        if target_dong:
            df = df[df['region_name'] == target_dong]
        return df

    # ì‚¬ìš©ì ì…ë ¥
    target_gu = '{gu_name}'
    target_dong = '{region}'

    # êµ¬/ë™ ì½”ë“œ ì¡°íšŒ
    dong_query = f"SELECT DISTINCT region_code FROM subcategory_avg_operating_period_stats WHERE region_name = '{target_dong}' LIMIT 1"
    dong_code = pd.read_sql(dong_query, engine).iloc[0]['region_code']
    print(f"ì„ íƒí•œ ë™ '{target_dong}'ì˜ ì§€ì—­ ì½”ë“œ: {dong_code}")

    # ê¸°ë³¸ í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°
    zone_df = load_table('zone_table')
    zone_df['zone_id'] = zone_df['zone_id'].astype(str)
    service_df = load_table('service_type')

    # ì§€í‘œ í…Œì´ë¸” ë¡œë”© ë° ìµœê·¼ 4ë¶„ê¸° í•„í„°ë§
    indicators = {
        'age': ('subcategory_avg_operating_period_stats', 'avg_operating_years_30'),
        'store': ('subcategory_store_count_stats', 'store_total'),
        'survive': ('subcategory_startup_survival', None),
        'openclose': ('subcategory_openclose_stats', None)
    }
    filtered_data = {}
    for key, (table, indicator) in indicators.items():
        df = query_table(table, indicator_val=indicator)
        if key in ['age', 'store']:
            filtered_data[key] = get_recent_quarters_by_category(df)[["category_small", "region_name", "region_code", "year", "quarter", "indicator", "value"]]
        elif key == 'survive':
            filtered_data[key] = get_recent_quarters_by_category(df)[["category_small", "region_name", "region_code", "year", "quarter",
                                                                      "survival_1yr", "survival_3yr", "survival_5yr"]]
        else:  # openclose
            filtered_data[key] = df[["category_small", "region_name", "region_code", "year", "quarter",
                                     "num_open", "num_close"]]

    # ë§¤ì¶œ ê´€ë ¨ ë°ì´í„° ì²˜ë¦¬
    years = [2022, 2023, 2024]
    gender_sales = {}
    gender_sales_test ={}
    age_sales = {}
    summary_sales = {}

    for year in years:
        st_ct_df = load_table(f"zone_store_count_{year}")
        gender_df = load_table(f"sales_by_gender_age_{year}")
        gender_known = gender_df[gender_df['gender'].isin(['ì—¬ì„±', 'ë‚¨ì„±'])]
        gender_unknown = gender_df[~gender_df['gender'].isin(['ì—¬ì„±', 'ë‚¨ì„±'])]

        gender_known = add_region_service_names(gender_known, zone_df, service_df, target_dong)
        gender_unknown = add_region_service_names(gender_unknown, zone_df, service_df, target_dong)

        #----gender
        gender_sales[year] = gender_known[["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "gender", "sales_amount"]] \
            .sort_values(by=["service_name", "gender", "year", "quarter"]).reset_index(drop=True)

        gender_sales[year] = gender_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        gender_sales[year].loc[:, 'avg_sales_per_store'] = gender_sales[year]['sales_amount'] / gender_sales[year]['count']

        gender_sales[year] = gender_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "gender", "avg_sales_per_store","count"]] \
            .sort_values(by=["service_name", "gender", "year", "quarter"]).reset_index(drop=True)

        #----age-group
        age_sales[year] = gender_unknown[["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "age_group", "sales_amount"]] \
            .sort_values(by=["service_name", "age_group", "year", "quarter"]).reset_index(drop=True)
        
        age_sales[year] = age_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        age_sales[year].loc[:, 'avg_sales_per_store'] = age_sales[year]['sales_amount'] / age_sales[year]['count']

        age_sales[year] = age_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "age_group", "avg_sales_per_store","count"]] \
            .sort_values(by=["service_name", "age_group", "year", "quarter"]).reset_index(drop=True)

        #----summary
        sales_df = query_sales(f"sales_summary_{year}")
        summary_sales[year] = sales_df[["region_name", "service_name", "zone_id", "service_code", "year", "quarter", "monthly_sales"]]
        summary_sales[year].loc[:, 'zone_id'] = summary_sales[year]['zone_id'].astype(str)

        summary_sales[year] = summary_sales[year].merge(st_ct_df[['zone_id', 'service_code', 'year', 'quarter','count']],on=['zone_id', 'service_code', 'year', 'quarter'],how='inner')
        summary_sales[year].loc[:, 'avg_sales_per_store'] = summary_sales[year]['monthly_sales'] / summary_sales[year]['count']

        summary_sales[year] = summary_sales[year][["region_name", "zone_id", "service_name", "service_code", "year", "quarter", "avg_sales_per_store","count"]] \
            .sort_values(by=["service_name", "year", "quarter"]).reset_index(drop=True)

    # ì¶œë ¥ ì˜ˆì‹œ
    for year in years:
        print(f"\nğŸ“Š ì—…ì¢… ì„±ë³„ ë§¤ì¶œ ì í¬ í‰ê· {year}\n", gender_sales[year])
        print(f"\nğŸ“Š ì—…ì¢… ì—°ë ¹ëŒ€ ë§¤ì¶œ {year}\n", age_sales[year])
        print(f"\nğŸ“Š ì—…ì¢… ë¶„ê¸°ë³„ ì›”ë§¤ì¶œ{year}\n", summary_sales[year])


    # í‰ê· ê°’ ê³„ì‚° í•¨ìˆ˜
    def get_avg(df, group_col='category_small', val_col=None, rename_col=None):
        df[val_col] = pd.to_numeric(df[val_col], errors='coerce')
        df = df.dropna(subset=[val_col])
        avg = df.groupby(group_col).agg({
            val_col: 'mean',
            'region_name': 'first'
        }).round(2).reset_index()
        if rename_col:
            avg = avg.rename(columns={val_col: rename_col})
        return avg

    # í‰ê·  ê³„ì‚°
    age_avg = get_avg(filtered_data['age'], val_col='value', rename_col='í‰ê· ì˜ì—…ê¸°ê°„(ë…„)')
    store_avg = get_avg(filtered_data['store'], val_col='value', rename_col='ì í¬ìˆ˜')

    survive_avg = filtered_data['survive'].groupby('category_small').agg({
        'survival_1yr': 'mean',
        'survival_3yr': 'mean',
        'survival_5yr': 'mean',
        'region_name': 'first'
    }).round(2).reset_index().rename(columns={
        'survival_1yr': '1ë…„ ìƒì¡´ìœ¨(%)',
        'survival_3yr': '3ë…„ ìƒì¡´ìœ¨(%)',
        'survival_5yr': '5ë…„ ìƒì¡´ìœ¨(%)'
    })

    openclose_avg = filtered_data['openclose'].groupby('category_small').agg({
        'num_open': 'mean',
        'num_close': 'mean',
        'region_name': 'first'
    }).round(2).reset_index().rename(columns={
        'num_open': 'í‰ê·  ê°œì—…ìˆ˜',
        'num_close': 'í‰ê·  íì—…ìˆ˜'
    })

    # ë³‘í•©
    dfs = [age_avg, store_avg, survive_avg, openclose_avg]
    from functools import reduce
    merged_df = reduce(lambda left, right: pd.merge(left, right, on=['category_small', 'region_name']), dfs)

    # ì»¬ëŸ¼ ì •ë¦¬
    merged_df = merged_df.rename(columns={
        'category_small': 'ì—…ì¢…ëª…',
        'region_name': 'í–‰ì •ë™ëª…'
    }).sort_values(by='ì í¬ìˆ˜', ascending=False).reset_index(drop=True)

    # ì¶œë ¥
    pd.set_option('display.float_format', '{:,.2f}'.format)
    print("ğŸ“Š ì¢…í•© ì§€ì—­ ìƒê¶Œ ìš”ì•½ ë¦¬í¬íŠ¸")
    print(merged_df)


    def get_avg_sales(sales_df, group_col):
        sales_df['avg_sales_per_store'] = pd.to_numeric(sales_df['avg_sales_per_store'], errors='coerce')
        sales_df = sales_df.dropna(subset=['avg_sales_per_store'])
        
        avg_df = sales_df.groupby(['service_code', group_col]).agg({
            'avg_sales_per_store': 'mean',
            'region_name': 'first',
            'service_name': 'first'
        }).round(2).reset_index()

        avg_df['avg_sales_per_store'] = (avg_df['avg_sales_per_store'] / 3).round(2)

        return avg_df

    def get_avg_sales_sum(sales_df, group_col=None):
        # ë§¤ì¶œ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜, ê²°ì¸¡ì¹˜ ì œê±°
        sales_df['avg_sales_per_store'] = pd.to_numeric(sales_df['avg_sales_per_store'], errors='coerce')
        sales_df = sales_df.dropna(subset=['avg_sales_per_store'])

        # ê¸°ë³¸ ê·¸ë£¹í•‘ ì»¬ëŸ¼: ì—…ì¢… ì½”ë“œ
        group_cols = ['service_code']
        #if group_col:
        #    group_cols.append(group_col)  # ì¶”ê°€ ê·¸ë£¹í•‘ ì»¬ëŸ¼ (ì˜ˆ: ì§€ì—­ì½”ë“œ ë“±)

        # ê·¸ë£¹ë³„ í‰ê·  ë§¤ì¶œ ê³„ì‚°
        avg_df = sales_df.groupby(group_cols).agg({
            'avg_sales_per_store': 'mean',     # ë¶„ê¸°ë³„ í‰ê·  ë§¤ì¶œ
            'region_name': 'first',            # ëŒ€í‘œ ì§€ì—­ëª… (group_colì´ ì§€ì—­ì¼ ê²½ìš°)
            'service_name': 'first'            # ì—…ì¢…ëª…
        })

        # ë¶„ê¸° ë§¤ì¶œ â†’ ì›”í‰ê·  ë§¤ì¶œë¡œ í™˜ì‚°
        avg_df['avg_sales_per_store'] = (avg_df['avg_sales_per_store'] / 3).round(2)

        return avg_df


    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    gender_list = []
    age_list = []
    summary_list = []

    for year in years:
        # ì„±ë³„ë³„ í‰ê· 
        gender_avg_df = get_avg_sales(gender_sales[year], group_col='gender')
        gender_avg_df['year'] = year
        gender_list.append(gender_avg_df)

        # ì—°ë ¹ëŒ€ë³„ í‰ê· 
        age_avg_df = get_avg_sales(age_sales[year], group_col='age_group')
        age_avg_df['year'] = year
        age_list.append(age_avg_df)

        # ì „ì²´ ì—…ì¢… í‰ê· 
        summary_avg_df = get_avg_sales_sum(summary_sales[year], group_col='service_code')
        summary_avg_df['year'] = year
        summary_list.append(summary_avg_df)

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹˜ê¸°
    gender_df = pd.concat(gender_list).reset_index(drop=True)
    age_df = pd.concat(age_list).reset_index(drop=True)
    summary_df = pd.concat(summary_list).reset_index(drop=True)

    # ì»¬ëŸ¼ëª… ì •ë¦¬
    gender_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'gender': 'ì„±ë³„',
        'avg_sales_per_store': 'ì„±ë³„ë³„ í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    age_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'age_group': 'ì—°ë ¹ëŒ€',
        'avg_sales_per_store': 'ì—°ë ¹ëŒ€ë³„ í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    summary_df.rename(columns={
        'region_name': 'í–‰ì •ë™ëª…',
        'service_name': 'ì—…ì¢…ëª…',
        'service_code': 'ì—…ì¢…ì½”ë“œ',
        'avg_sales_per_store': 'í‰ê·  ì›” ë§¤ì¶œ'
    }, inplace=True)

    gender_df.to_json('gender_avg_sales_industry.json', orient='records', force_ascii=False, indent=4)
    age_df.to_json('age_avg_sales_industry.json', orient='records', force_ascii=False, indent=4)
    summary_df.to_json('summary_avg_sales_industry.json', orient='records', force_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ ë³€ìˆ˜ì— ì €ì¥
    gender_industry_json = gender_df.to_json(orient='records', force_ascii=False, indent=4)
    age_industry_json = age_df.to_json(orient='records', force_ascii=False, indent=4)
    summary_industry_json = summary_df.to_json(orient='records', force_ascii=False, indent=4)

    # ------------------------
    # ì •ê·œí™” ëŒ€ìƒ ì»¬ëŸ¼ ì •ì˜
    # ------------------------
    score_columns = [
        'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)', 'ì í¬ìˆ˜',
        '1ë…„ ìƒì¡´ìœ¨(%)', '3ë…„ ìƒì¡´ìœ¨(%)', '5ë…„ ìƒì¡´ìœ¨(%)',
        'í‰ê·  ê°œì—…ìˆ˜', 'í‰ê·  íì—…ìˆ˜'
    ]
    score_columns_2 = ['2022_í‰ê· ë§¤ì¶œ', '2023_í‰ê· ë§¤ì¶œ', '2024_í‰ê· ë§¤ì¶œ']

    # ------------------------
    # ê°€ì¤‘ì¹˜ ì •ì˜ (ìˆ˜ì • ê°€ëŠ¥)
    # ------------------------
    weights = {
        'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)': 0.05,
        'ì í¬ìˆ˜': 0.15,
        '1ë…„ ìƒì¡´ìœ¨(%)': 0.05,
        '3ë…„ ìƒì¡´ìœ¨(%)': 0.07,
        '5ë…„ ìƒì¡´ìœ¨(%)': 0.10,
        'í‰ê·  ê°œì—…ìˆ˜': 0.04,
        'í‰ê·  íì—…ìˆ˜': -0.04,
        '2022_í‰ê· ë§¤ì¶œ': 0.15,
        '2023_í‰ê· ë§¤ì¶œ': 0.17,
        '2024_í‰ê· ë§¤ì¶œ': 0.22
    }

    # ------------------------
    # ì •ê·œí™” ìˆ˜í–‰
    # ------------------------
    # summary_dfì—ì„œ ì—°ë„ë³„ í‰ê·  ë§¤ì¶œ í”¼ë²— (ì—…ì¢…/í–‰ì •ë™ ê¸°ì¤€ìœ¼ë¡œ wide format ë§Œë“¤ê¸°)
    pivot_summary = summary_df.pivot_table(
        index=['í–‰ì •ë™ëª…', 'ì—…ì¢…ëª…'],
        columns='year',
        values='í‰ê·  ì›” ë§¤ì¶œ',
        aggfunc='mean'
    ).reset_index()

    # ì»¬ëŸ¼ëª… ì •ë¦¬ (2022, 2023, 2024 â†’ '2022_í‰ê· ë§¤ì¶œ' í˜•íƒœë¡œ)
    pivot_summary.columns.name = None
    pivot_summary.rename(columns={
        2022: '2022_í‰ê· ë§¤ì¶œ',
        2023: '2023_í‰ê· ë§¤ì¶œ',
        2024: '2024_í‰ê· ë§¤ì¶œ'
    }, inplace=True)

    # merged_dfì— í‰ê·  ë§¤ì¶œ ë³‘í•©
    merged_df = merged_df.merge(pivot_summary, on=['ì—…ì¢…ëª…','í–‰ì •ë™ëª…'], how='left')
    merged_df.to_json('merged_industry.json', orient='records', force_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ ë³€ìˆ˜ì— ì €ì¥
    merged_industry_json = merged_df.to_json(orient='records', force_ascii=False, indent=4)

    # ì •ê·œí™” ëŒ€ìƒ ì»¬ëŸ¼
    all_score_columns = score_columns + score_columns_2

    # ì •ê·œí™” ìˆ˜í–‰
    clean_df = merged_df[all_score_columns].replace([np.inf, -np.inf], np.nan).fillna(0)

    scaler = MinMaxScaler()
    normalized = scaler.fit_transform(clean_df)
    normalized_df = pd.DataFrame(normalized, columns=[f'norm_{col}' for col in all_score_columns])
    # ë³‘í•©
    merged_with_norm = pd.concat([merged_df, normalized_df], axis=1)

    # ì ìˆ˜ ê³„ì‚°
    merged_with_norm['ì—…ì¢…_ì¶”ì²œì ìˆ˜'] = sum(
        merged_with_norm[f'norm_{col}'] * weight
        for col, weight in weights.items()
    )

    # ë¶ˆí•„ìš”í•œ ì •ê·œí™” ì»¬ëŸ¼ ì œê±°, ì •ë ¬
    norm_cols = [f'norm_{col}' for col in all_score_columns]
    final_result = merged_with_norm.drop(columns=norm_cols).sort_values(by='ì—…ì¢…_ì¶”ì²œì ìˆ˜', ascending=False).reset_index(drop=True)
    final_result = final_result.replace([np.inf, -np.inf], np.nan)
    final_result = final_result.dropna(subset=['2022_í‰ê· ë§¤ì¶œ', '2023_í‰ê· ë§¤ì¶œ', '2024_í‰ê· ë§¤ì¶œ'])

    # ì €ì¥
    final_result.to_csv('filtered_result_industry.csv', index=False, encoding='utf-8-sig')
    final_result.to_json('filtered_result_industry.json', orient='records', force_ascii=False, indent=4)
    filtered_result_industry_json = final_result.to_json(orient='records', force_ascii=False, indent=4)

    # ì¶œë ¥
    print("ğŸ† ìµœì¢… ì—…ì¢… ì¶”ì²œ ê²°ê³¼ (ì§€ì—­+ì—…ì¢… ê¸°ì¤€)")
    print(final_result[['í–‰ì •ë™ëª…', 'ì—…ì¢…ëª…', 'ì—…ì¢…_ì¶”ì²œì ìˆ˜']].head(10))

    #----LLM----
    # API í‚¤ ì„¤ì •
    genai.configure(api_key="AIzaSyCiEbjep2f6PRLqTr1JKYE2vMlbrAHvr-E")

    # ëª¨ë¸ ì„ íƒ
    model = genai.GenerativeModel(model_name="models/gemini-2.5-flash")
    def generate_business_summary(row, target_dong):
        ì—…ì¢…ëª… = row['ì—…ì¢…ëª…']
        í–‰ì •ë™ëª… = row['í–‰ì •ë™ëª…']

        few_shot_examples ="""
    [ì˜ˆì‹œ 1]
    - ì—…ì¢…ëª…: ì»¤í”¼ ì „ë¬¸ì 
    - í‰ê· ì˜ì—…ê¸°ê°„: 4.2ë…„
    - ì í¬ìˆ˜: 180ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: 72%
    - 5ë…„ ìƒì¡´ìœ¨: 55%
    - í‰ê·  ê°œì—…ìˆ˜: 14ê°œ
    - í‰ê·  íì—…ìˆ˜: 8ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: 8,400,000ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: 8,800,000ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: 9,200,000ì›

    ê¾¸ì¤€í•œ ë§¤ì¶œ ìƒìŠ¹ê³¼ ë†’ì€ ìƒì¡´ìœ¨ì´ ë‹ë³´ì´ë©°, ì°½ì—…ì— ì•ˆì •ì ì¸ ì—…ì¢…ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.

    [ì˜ˆì‹œ 2]
    - ì—…ì¢…ëª…: ë¶„ì‹ ì „ë¬¸ì 
    - í‰ê· ì˜ì—…ê¸°ê°„: 3.6ë…„
    - ì í¬ìˆ˜: 90ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: 69%
    - 5ë…„ ìƒì¡´ìœ¨: 51%
    - í‰ê·  ê°œì—…ìˆ˜: 12ê°œ
    - í‰ê·  íì—…ìˆ˜: 6ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: 6,100,000ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: 6,400,000ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: 6,900,000ì›

    ë§¤ì¶œì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, ë¹„êµì  ë‚®ì€ íì—…ë¥ ë¡œ ì•ˆì •ì ì¸ ì°½ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """

        # í”„ë¡¬í”„íŠ¸ ì™„ì„±
        prompt = f"""
    ë‹¹ì‹ ì€ ì—…ì¢… ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” '{target_dong}' ì§€ì—­ì—ì„œ '{ì—…ì¢…ëª…}' ì—…ì¢…ì„ '{í–‰ì •ë™ëª…}' ë‚´ì— ì°½ì—…í–ˆì„ ë•Œì˜ ì£¼ìš” ì§€í‘œì…ë‹ˆë‹¤:

    ë¨¼ì € ì°¸ê³ ìš© ì˜ˆì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”:
    {few_shot_examples}

    ---

    ì´ì œ ì•„ë˜ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ì¢… ì¶”ì²œ ì‚¬ìœ ë¥¼ ê¸ì •ì ìœ¼ë¡œ **3ì¤„ ì´ë‚´ë¡œ ì‘ì„±**í•´ ì£¼ì„¸ìš”:


    - í‰ê· ì˜ì—…ê¸°ê°„: {row['í‰ê· ì˜ì—…ê¸°ê°„(ë…„)']}ë…„
    - ì í¬ìˆ˜: {row['ì í¬ìˆ˜']}ê°œ
    - 3ë…„ ìƒì¡´ìœ¨: {row['3ë…„ ìƒì¡´ìœ¨(%)']}%
    - 5ë…„ ìƒì¡´ìœ¨: {row['5ë…„ ìƒì¡´ìœ¨(%)']}%
    - í‰ê·  ê°œì—…ìˆ˜: {row['í‰ê·  ê°œì—…ìˆ˜']}ê°œ
    - í‰ê·  íì—…ìˆ˜: {row['í‰ê·  íì—…ìˆ˜']}ê°œ
    - 2022 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2022_í‰ê· ë§¤ì¶œ']):,}ì›
    - 2023 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2023_í‰ê· ë§¤ì¶œ']):,}ì›
    - 2024 í‰ê·  ì›”ë§¤ì¶œ: {int(row['2024_í‰ê· ë§¤ì¶œ']):,}ì›

    """
        response = model.generate_content(prompt)
        return response.text.strip()

    # ----ì¶”ì²œ ê²°ê³¼ ìƒì„±----
    recommendation_list = []

    subcategory_df = pd.read_sql("SELECT DISTINCT category_large, category_small FROM subcategory_store_count_stats", engine)

    # ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ
    top_n = 5

    for idx, row in final_result.head(top_n).iterrows():
        label = row['ì—…ì¢…ëª…']

        # ëŒ€ë¶„ë¥˜(category_large) ì°¾ê¸°
        match_row = subcategory_df[subcategory_df['category_small'] == label]
        if not match_row.empty:
            large_label = match_row.iloc[0]['category_large']
        else:
            large_label = 'ê¸°íƒ€'  # í˜¹ì€ None ë“± ê¸°ë³¸ê°’ ì„¤ì •

        reason = generate_business_summary(row, target_dong)
        recommendation_list.append({
            'category_large': large_label,
            'category_small': label,
            'reason': reason
        })

    # ----JSON ì €ì¥----
    recommendation_dict = {
        target_dong: recommendation_list
    }

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open('recommendation_industry.json', 'w', encoding='utf-8') as f:
        json.dump(recommendation_dict, f, ensure_ascii=False, indent=4)

    # JSON ë¬¸ìì—´ë¡œ ë³€ìˆ˜ì— ì €ì¥
    recommendation_industry_json = json.dumps(recommendation_dict, ensure_ascii=False, indent=4)

    # ì¶œë ¥
    print(recommendation_industry_json)
