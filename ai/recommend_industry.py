# ai/recommend_industry.py
import os
import time
import json
import difflib
import numpy as np
import pandas as pd
from sqlalchemy import text
from sklearn.preprocessing import MinMaxScaler

# DB/ì—”ì§„ì€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
from config.settings import get_engine

# ì„ íƒ: LLM
try:
    import google.generativeai as genai
    from google.api_core.exceptions import ResourceExhausted, GoogleAPIError
except Exception:
    genai = None
    ResourceExhausted = GoogleAPIError = Exception

# ====== í™˜ê²½ì„¤ì • ======
USE_LLM = os.getenv("USE_LLM", "1") == "1"
TOPK_FOR_REASON = int(os.getenv("TOPK_FOR_REASON", "5"))
REASON_CACHE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "reason_cache.json"))

# ====== LLM (Gemini) ë¡œë”© (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í´ë°±) ======
_genai_available = False
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
if USE_LLM and genai and GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        _genai_available = True
    except Exception:
        _genai_available = False

def _genai_model(model_name: str):
    if not _genai_available:
        return None
    try:
        # google-generativeaiëŠ” model_name í‚¤ì›Œë“œ ì‚¬ìš©
        return genai.GenerativeModel(model_name=model_name)
    except Exception:
        return None

# ====== ê°„ë‹¨ íŒŒì¼ ìºì‹œ ======
def _load_reason_cache():
    if os.path.exists(REASON_CACHE_PATH):
        try:
            with open(REASON_CACHE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def _save_reason_cache(cache):
    os.makedirs(os.path.dirname(REASON_CACHE_PATH), exist_ok=True)
    with open(REASON_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

_REASON_CACHE = _load_reason_cache()

def _cache_key(gu_name, region, cat_small, date_key):
    return f"{gu_name}::{region}::{cat_small}::{date_key}"

# ====== ê·œì¹™ ê¸°ë°˜ ì´ìœ  (LLM í´ë°±ìš©) ======
def rule_based_reason(row):
    def _num(x, d=0):
        try:
            return round(float(x), d)
        except Exception:
            return 0
    def _int(x):
        try:
            return int(float(x))
        except Exception:
            return 0

    parts = []
    parts.append(f"{row.get('ì—…ì¢…ëª…','í•´ë‹¹ ì—…ì¢…')} ì—…ì¢…ì€ ì í¬ìˆ˜ {_int(row.get('ì í¬ìˆ˜',0))}ê°œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    gy = _num(row.get('í‰ê· ì˜ì—…ê¸°ê°„(ë…„)', 0), 2)
    if gy > 0:
        parts.append(f"í‰ê·  ì˜ì—…ê¸°ê°„ì´ ì•½ {gy}ë…„ìœ¼ë¡œ ì•ˆì •ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    for y in (2024, 2023, 2022):
        col = f"{y}_í‰ê· ë§¤ì¶œ"
        val = _num(row.get(col, 0), 0)
        if val > 0:
            parts.append(f"{y}ë…„ í‰ê·  ë§¤ì¶œì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
            break
    s3 = _num(row.get('3ë…„ ìƒì¡´ìœ¨(%)', 0), 1)
    if s3 > 0:
        parts.append(f"3ë…„ ìƒì¡´ìœ¨ {s3}% ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    if not parts:
        return "ì§€ì—­ ìˆ˜ìš”ì™€ ê²½ìŸ ìƒí™©ì„ ê³ ë ¤í•  ë•Œ ì ì¬ë ¥ì´ ìˆëŠ” ì—…ì¢…ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."
    return " ".join(parts)

# ====== LLM ì´ìœ  ìƒì„± (429/ì˜¤ë¥˜ â†’ í´ë°±/ìºì‹œ) ======
def generate_reason_with_llm(gu_name, region, row, prefer_model="models/gemini-1.5-flash"):
    date_key = time.strftime("%Y-%m-%d")
    key = _cache_key(gu_name, region, row.get('ì—…ì¢…ëª…', ''), date_key)
    if key in _REASON_CACHE:
        return _REASON_CACHE[key], "cache"

    if not _genai_available:
        r = rule_based_reason(row)
        _REASON_CACHE[key] = r; _save_reason_cache(_REASON_CACHE)
        return r, "fallback-disabled"

    prompt = f"""
ë‹¹ì‹ ì€ ìƒê¶Œ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ì¢… ì¶”ì²œ ì‚¬ìœ ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì¨ì£¼ì„¸ìš”.
- í–‰ì •ë™: {region} ({gu_name})
- ì—…ì¢…ëª…: {row.get('ì—…ì¢…ëª…')}
- ì í¬ìˆ˜: {row.get('ì í¬ìˆ˜')}
- í‰ê· ì˜ì—…ê¸°ê°„(ë…„): {row.get('í‰ê· ì˜ì—…ê¸°ê°„(ë…„)')}
- 3ë…„ ìƒì¡´ìœ¨(%): {row.get('3ë…„ ìƒì¡´ìœ¨(%)')}
- 5ë…„ ìƒì¡´ìœ¨(%): {row.get('5ë…„ ìƒì¡´ìœ¨(%)')}
- 2022~2024 í‰ê· ë§¤ì¶œ(ê°€ëŠ¥í•œ ì—°ë„ë§Œ): {row.get('2022_í‰ê· ë§¤ì¶œ','')}, {row.get('2023_í‰ê· ë§¤ì¶œ','')}, {row.get('2024_í‰ê· ë§¤ì¶œ','')}
ê°€ì´ë“œ: ìˆ˜ì¹˜ê°€ í´ìˆ˜ë¡ ê¸ì •ì , 0 ë˜ëŠ” ê²°ì¸¡ì€ ì–¸ê¸‰í•˜ì§€ ì•Šì•„ë„ ë¨. ê³¼ì¥ í‘œí˜„ ê¸ˆì§€, ê°„ë‹¨ëª…ë£Œ, í•œêµ­ì–´.
"""

    models_try = [prefer_model, "models/gemini-1.5-flash-8b"]
    for m in models_try:
        try:
            model = _genai_model(m)
            if not model:
                raise RuntimeError("LLM not configured")
            resp = model.generate_content(prompt)
            text = (resp.text or "").strip()
            if not text:
                raise RuntimeError("Empty LLM response")
            _REASON_CACHE[key] = text; _save_reason_cache(_REASON_CACHE)
            return text, m
        except (ResourceExhausted, GoogleAPIError, Exception) as e:
            msg = str(e)
            if "429" in msg or isinstance(e, ResourceExhausted):
                time.sleep(1.5)
                try:
                    model = _genai_model(m)
                    if model:
                        resp = model.generate_content(prompt)
                        text = (resp.text or "").strip()
                        if text:
                            _REASON_CACHE[key] = text; _save_reason_cache(_REASON_CACHE)
                            return text, f"{m}-retry"
                except Exception:
                    pass
            continue

    r = rule_based_reason(row)
    _REASON_CACHE[key] = r; _save_reason_cache(_REASON_CACHE)
    return r, "fallback-429"

# ====== í•µì‹¬ ì¶”ì²œ íŒŒì´í”„ë¼ì¸ ======
def run_industry_recommendation(region, gu_name):
    """
    ì…ë ¥: region(í–‰ì •ë™ëª…), gu_name(êµ¬ëª…)
    ì¶œë ¥: back/cache/runtime/recommendation_industry.json ì— {region: [{category_large, category_small, reason}, ...]} ì €ì¥
    """
    engine = get_engine()

    # (ì„ íƒ) ì—°ê²° í™•ì¸
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("âœ… DB ì—°ê²° í™•ì¸")
    except Exception as e:
        print("âŒ DB ì—°ê²° ì‹¤íŒ¨:", e)

    def load_table(table_name: str) -> pd.DataFrame:
        return pd.read_sql(f"SELECT * FROM {table_name}", engine)

    def load_or_cache(name, table):
        os.makedirs("cache", exist_ok=True)
        path = f"cache/{name}.feather"
        if os.path.exists(path):
            print(f"ğŸ“‚ ìºì‹œ ë¶ˆëŸ¬ì˜´: {name}")
            return pd.read_feather(path)
        df = load_table(table)
        df.to_feather(path)
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥: {name}")
        return df

    def get_recent_quarters_by_category(df, group_cols=['category_small'], num_quarters=4):
        if df.empty:
            return df
        df_sorted = df.sort_values(by=group_cols + ['year', 'quarter'])
        return df_sorted.groupby(group_cols, group_keys=False).tail(num_quarters)

    # region_code ì¡°íšŒ (íŒŒë¼ë¯¸í„° ë°”ì¸ë”©)
    dong_code_df = pd.read_sql(
        "SELECT DISTINCT region_code FROM subcategory_avg_operating_period_stats WHERE region_name = %s LIMIT 1",
        engine, params=(region,),
    )
    if dong_code_df.empty:
        raise ValueError(f"region_codeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {region}")
    dong_code = dong_code_df.iloc[0]['region_code']
    print(f"ì„ íƒí•œ ë™ '{region}'ì˜ ì§€ì—­ ì½”ë“œ: {dong_code}")

    # ì§€í‘œ ë¡œë“œ
    indicators = {
        'age': ('subcategory_avg_operating_period_stats', 'avg_operating_years_30'),
        'store': ('subcategory_store_count_stats', 'store_total'),
        'survive': ('subcategory_startup_survival', None),
        'openclose': ('subcategory_openclose_stats', None)
    }
    raw_data = {k: load_or_cache(f"{k}_all_df", t) for k, (t, _) in indicators.items()}

    def filter_by_region(df, region_name):
        return df[df['region_name'] == region_name].reset_index(drop=True)

    filtered_data = {}
    for key, (_, indicator) in indicators.items():
        df_region = filter_by_region(raw_data[key], region)
        if indicator and 'indicator' in df_region.columns:
            df_region = df_region[df_region['indicator'] == indicator]
        if key in ['age', 'store']:
            cols = ["category_small", "region_name", "region_code", "year", "quarter", "indicator", "value"]
            filtered_data[key] = get_recent_quarters_by_category(df_region)[cols] if not df_region.empty else df_region
        elif key == 'survive':
            cols = ["category_small", "region_name", "region_code", "year", "quarter",
                    "survival_1yr", "survival_3yr", "survival_5yr"]
            filtered_data[key] = get_recent_quarters_by_category(df_region)[cols] if not df_region.empty else df_region
        else:  # openclose
            cols = ["category_small", "region_name", "region_code", "year", "quarter", "num_open", "num_close"]
            filtered_data[key] = df_region[cols] if not df_region.empty else df_region

    # í‰ê·  ê³„ì‚° ìœ í‹¸
    def get_avg(df, group_col='category_small', val_col=None, rename_col=None):
        if df.empty or (val_col not in df.columns):
            return pd.DataFrame(columns=[group_col, 'region_name', rename_col or val_col])
        df[val_col] = pd.to_numeric(df[val_col], errors='coerce')
        df = df.dropna(subset=[val_col])
        avg = df.groupby(group_col).agg({val_col: 'mean', 'region_name': 'first'}).round(2).reset_index()
        if rename_col:
            avg = avg.rename(columns={val_col: rename_col})
        return avg

    age_avg   = get_avg(filtered_data['age'],   val_col='value', rename_col='í‰ê· ì˜ì—…ê¸°ê°„(ë…„)')
    store_avg = get_avg(filtered_data['store'], val_col='value', rename_col='ì í¬ìˆ˜')

    if not filtered_data['survive'].empty:
        survive_avg = filtered_data['survive'].groupby('category_small').agg({
            'survival_1yr': 'mean',
            'survival_3yr': 'mean',
            'survival_5yr': 'mean',
            'region_name': 'first'
        }).round(2).reset_index().rename(columns={
            'survival_1yr': '1ë…„ ìƒì¡´ìœ¨(%)',
            'survival_3yr': '3ë…„ ìƒì¡´ìœ¨(%)',
            'survival_5yr': '5ë…„ ìƒì¡´ìœ¨(%)'
        })
    else:
        survive_avg = pd.DataFrame(columns=['category_small','region_name','1ë…„ ìƒì¡´ìœ¨(%)','3ë…„ ìƒì¡´ìœ¨(%)','5ë…„ ìƒì¡´ìœ¨(%)'])

    if not filtered_data['openclose'].empty:
        openclose_avg = filtered_data['openclose'].groupby('category_small').agg({
            'num_open': 'mean',
            'num_close': 'mean',
            'region_name': 'first'
        }).round(2).reset_index().rename(columns={
            'num_open': 'í‰ê·  ê°œì—…ìˆ˜',
            'num_close': 'í‰ê·  íì—…ìˆ˜'
        })
    else:
        openclose_avg = pd.DataFrame(columns=['category_small','region_name','í‰ê·  ê°œì—…ìˆ˜','í‰ê·  íì—…ìˆ˜'])

    dfs = [age_avg, store_avg, survive_avg, openclose_avg]
    merged_df = dfs[0]
    for d in dfs[1:]:
        merged_df = pd.merge(merged_df, d, on=['category_small','region_name'], how='outer')

    merged_df = merged_df.rename(columns={'category_small':'ì—…ì¢…ëª…','region_name':'í–‰ì •ë™ëª…'})
    merged_df[['í‰ê· ì˜ì—…ê¸°ê°„(ë…„)','ì í¬ìˆ˜','1ë…„ ìƒì¡´ìœ¨(%)','3ë…„ ìƒì¡´ìœ¨(%)','5ë…„ ìƒì¡´ìœ¨(%)','í‰ê·  ê°œì—…ìˆ˜','í‰ê·  íì—…ìˆ˜']] = \
        merged_df[['í‰ê· ì˜ì—…ê¸°ê°„(ë…„)','ì í¬ìˆ˜','1ë…„ ìƒì¡´ìœ¨(%)','3ë…„ ìƒì¡´ìœ¨(%)','5ë…„ ìƒì¡´ìœ¨(%)','í‰ê·  ê°œì—…ìˆ˜','í‰ê·  íì—…ìˆ˜']].fillna(0)

    # === ë§¤ì¶œ ì²˜ë¦¬ ===
    def add_region_service_names(df, zone_df, service_df, region=None):
        if df.empty:
            return df
        df['zone_id'] = df['zone_id'].astype(str)
        zone_df['zone_id'] = zone_df['zone_id'].astype(str)
        df = df.merge(zone_df, on='zone_id', how='left', suffixes=('', '_zone'))
        df = df.merge(service_df, on='service_code', how='left', suffixes=('', '_service'))
        if 'region_name_zone' in df.columns:
            df['region_name'] = df['region_name_zone']; df.drop(columns=['region_name_zone'], inplace=True)
        elif 'region_name_service' in df.columns:
            df['region_name'] = df['region_name_service']; df.drop(columns=['region_name_service'], inplace=True)
        if region:
            df = df[df['region_name'] == region]
        # service_name ê°œí–‰ ì •ë¦¬
        if 'service_name' in df.columns:
            df['service_name'] = df['service_name'].astype(str).str.replace(r'[\r\n]+', '', regex=True)
        return df

    zone_df = load_table('zone_table')
    service_df = load_table('service_type')

    def load_year(table_prefix, year):
        return pd.read_sql(f"SELECT * FROM {table_prefix}_{year}", engine)

    def preprocess_sales(year, region, table_name, filter_cols, group_cols=None):
        df = load_year(table_name, year)
        st_ct_df = load_year("zone_store_count", year)
        df = add_region_service_names(df, zone_df, service_df, region)
        if df.empty:
            return df
        df = df[filter_cols]
        if group_cols is None:
            group_cols = []
        df = df.sort_values(by=group_cols + ['year','quarter']).reset_index(drop=True)
        df = df.merge(st_ct_df[['zone_id','service_code','year','quarter','count']],
                      on=['zone_id','service_code','year','quarter'], how='inner')
        # ì»¬ëŸ¼ ìë™ íƒìƒ‰
        sales_col = [c for c in filter_cols if 'sales' in c or 'amount' in c][-1]
        df['avg_sales_per_store'] = df[sales_col] / df['count']
        return df[['region_name','zone_id','service_name','service_code','year','quarter','avg_sales_per_store','count']]

    years = [2022, 2023, 2024]
    summary_sales = {y: preprocess_sales(
        y, region,
        table_name='sales_summary',
        filter_cols=["region_name","zone_id","service_name","service_code","year","quarter","monthly_sales"],
        group_cols=["service_name"]
    ) for y in years}

    def get_avg_sales_sum(sales_df):
        if sales_df.empty:
            return pd.DataFrame(columns=['service_code','avg_sales_per_store','region_name','service_name'])
        sales_df['avg_sales_per_store'] = pd.to_numeric(sales_df['avg_sales_per_store'], errors='coerce')
        sales_df = sales_df.dropna(subset=['avg_sales_per_store'])
        avg_df = sales_df.groupby(['service_code']).agg({
            'avg_sales_per_store': 'mean',
            'region_name': 'first',
            'service_name': 'first'
        }).reset_index()
        avg_df['avg_sales_per_store'] = (avg_df['avg_sales_per_store'] / 3).round(2)
        return avg_df

    summary_list = []
    for y in years:
        df = get_avg_sales_sum(summary_sales[y])
        df['year'] = y
        summary_list.append(df)
    summary_df = pd.concat(summary_list, ignore_index=True) if summary_list else pd.DataFrame()

    if not summary_df.empty:
        summary_df.rename(columns={
            'region_name':'í–‰ì •ë™ëª…','service_name':'ì—…ì¢…ëª…','service_code':'ì—…ì¢…ì½”ë“œ',
            'avg_sales_per_store':'í‰ê·  ì›” ë§¤ì¶œ'
        }, inplace=True)
        pivot_summary = summary_df.pivot_table(
            index=['í–‰ì •ë™ëª…','ì—…ì¢…ëª…'], columns='year', values='í‰ê·  ì›” ë§¤ì¶œ', aggfunc='mean'
        ).reset_index()
        pivot_summary.columns.name = None
        pivot_summary.rename(columns={2022:'2022_í‰ê· ë§¤ì¶œ', 2023:'2023_í‰ê· ë§¤ì¶œ', 2024:'2024_í‰ê· ë§¤ì¶œ'}, inplace=True)
        merged_df = merged_df.merge(pivot_summary, on=['ì—…ì¢…ëª…','í–‰ì •ë™ëª…'], how='left')
    else:
        for c in ['2022_í‰ê· ë§¤ì¶œ','2023_í‰ê· ë§¤ì¶œ','2024_í‰ê· ë§¤ì¶œ']:
            merged_df[c] = 0.0

    for c in ['2022_í‰ê· ë§¤ì¶œ','2023_í‰ê· ë§¤ì¶œ','2024_í‰ê· ë§¤ì¶œ']:
        if c not in merged_df.columns:
            merged_df[c] = 0.0
        merged_df[c] = pd.to_numeric(merged_df[c], errors='coerce').fillna(0.0)

    # ===== ìŠ¤ì½”ì–´ë§ =====
    score_columns = [
        'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)','ì í¬ìˆ˜','1ë…„ ìƒì¡´ìœ¨(%)','3ë…„ ìƒì¡´ìœ¨(%)','5ë…„ ìƒì¡´ìœ¨(%)','í‰ê·  ê°œì—…ìˆ˜','í‰ê·  íì—…ìˆ˜',
        '2022_í‰ê· ë§¤ì¶œ','2023_í‰ê· ë§¤ì¶œ','2024_í‰ê· ë§¤ì¶œ'
    ]
    weights = {
        'í‰ê· ì˜ì—…ê¸°ê°„(ë…„)': 0.05, 'ì í¬ìˆ˜': 0.15, '1ë…„ ìƒì¡´ìœ¨(%)': 0.05, '3ë…„ ìƒì¡´ìœ¨(%)': 0.07,
        '5ë…„ ìƒì¡´ìœ¨(%)': 0.10, 'í‰ê·  ê°œì—…ìˆ˜': 0.04, 'í‰ê·  íì—…ìˆ˜': -0.04,
        '2022_í‰ê· ë§¤ì¶œ': 0.15, '2023_í‰ê· ë§¤ì¶œ': 0.17, '2024_í‰ê· ë§¤ì¶œ': 0.22
    }

    clean_df = merged_df[score_columns].replace([np.inf,-np.inf], np.nan).fillna(0)
    scaler = MinMaxScaler()
    normalized = scaler.fit_transform(clean_df)
    normalized_df = pd.DataFrame(normalized, columns=[f"norm_{c}" for c in score_columns])
    merged_with_norm = pd.concat([merged_df, normalized_df], axis=1)

    merged_with_norm['ì—…ì¢…_ì¶”ì²œì ìˆ˜'] = 0.0
    for col, w in weights.items():
        merged_with_norm['ì—…ì¢…_ì¶”ì²œì ìˆ˜'] += merged_with_norm[f"norm_{col}"] * w

    final_result = merged_with_norm.drop(columns=[f"norm_{c}" for c in score_columns]).sort_values(
        by='ì—…ì¢…_ì¶”ì²œì ìˆ˜', ascending=False
    ).reset_index(drop=True)

    print("ğŸ“Š ì¢…í•© ì§€ì—­ ìƒê¶Œ ìš”ì•½ ë¦¬í¬íŠ¸")
    print(merged_df)
    print("ğŸ† ìµœì¢… ì—…ì¢… ì¶”ì²œ ê²°ê³¼ (ì§€ì—­+ì—…ì¢… ê¸°ì¤€)")
    print(final_result[['í–‰ì •ë™ëª…','ì—…ì¢…ëª…','ì—…ì¢…_ì¶”ì²œì ìˆ˜']].head(10))

    # ===== ìƒìœ„ TOPKì— ëŒ€í•´ ì´ìœ  ìƒì„± =====
    subcategory_df = pd.read_sql(
        "SELECT DISTINCT category_large, category_small FROM subcategory_store_count_stats", engine
    )

    recommendations = []
    for _, row in final_result.head(TOPK_FOR_REASON).iterrows():
        label = (row.get('ì—…ì¢…ëª…') or '').strip().replace('\r','')
        large_label = 'ê¸°íƒ€'
        mr = subcategory_df[subcategory_df['category_small'] == label]
        if not mr.empty:
            large_label = mr.iloc[0]['category_large']

        reason, src = generate_reason_with_llm(gu_name, region, row)
        recommendations.append({
            'category_large': large_label,
            'category_small': label,
            'reason': reason
        })

    # ===== JSON ì €ì¥ (reloader ê°ì‹œ ë°– ê²½ë¡œ) =====
    runtime_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'cache', 'runtime'))
    os.makedirs(runtime_dir, exist_ok=True)
    out_path = os.path.join(runtime_dir, 'recommendation_industry.json')

    payload = { region: recommendations }
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=4)

    print(json.dumps(payload, ensure_ascii=False, indent=4))
    return payload
